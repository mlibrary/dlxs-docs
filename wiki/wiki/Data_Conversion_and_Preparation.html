<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
	
<!-- Mirrored from localhost/wiki/Data_Conversion_and_Preparation by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 03 Feb 2025 00:25:48 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<meta name="generator" content="MediaWiki 1.13.5" />
		<meta name="keywords" content="Data Conversion and Preparation,DLXS Wiki,Mounting a Finding Aids Collection,Working with Unicode,Working with XML &amp; XSLT" />
		<link rel="shortcut icon" href="http://localhost/favicon.ico" />
		<link rel="search" type="application/opensearchdescription+xml" href="http://localhost/mediawiki/opensearch_desc.php" title="DLXS Documentation (en)" />
		<link rel="alternate" type="application/rss+xml" title="DLXS Documentation RSS Feed" href="http://localhost/mediawiki/index.php?title=Special:RecentChanges&amp;feed=rss" />
		<link rel="alternate" type="application/atom+xml" title="DLXS Documentation Atom Feed" href="http://localhost/mediawiki/index.php?title=Special:RecentChanges&amp;feed=atom" />
		<title>Data Conversion and Preparation - DLXS Documentation</title>
		<style type="text/css" media="screen, projection">/*<![CDATA[*/
			@import "../mediawiki/skins/common/sharedfa7c.css?164";
			@import "../mediawiki/skins/monobook/mainfa7c.css?164";
		/*]]>*/</style>
		<link rel="stylesheet" type="text/css" media="print" href="../mediawiki/skins/common/commonPrintfa7c.css?164" />
		<!--[if lt IE 5.5000]><style type="text/css">@import "/mediawiki/skins/monobook/IE50Fixes.css?164";</style><![endif]-->
		<!--[if IE 5.5000]><style type="text/css">@import "/mediawiki/skins/monobook/IE55Fixes.css?164";</style><![endif]-->
		<!--[if IE 6]><style type="text/css">@import "/mediawiki/skins/monobook/IE60Fixes.css?164";</style><![endif]-->
		<!--[if IE 7]><style type="text/css">@import "/mediawiki/skins/monobook/IE70Fixes.css?164";</style><![endif]-->
		<!--[if lt IE 7]><script type="text/javascript" src="/mediawiki/skins/common/IEFixes.js?164"></script>
		<meta http-equiv="imagetoolbar" content="no" /><![endif]-->
		
		<script type= "text/javascript">/*<![CDATA[*/
var skin = "monobook";
var stylepath = "/mediawiki/skins";
var wgArticlePath = "/wiki/$1";
var wgScriptPath = "/mediawiki";
var wgScript = "http://localhost/mediawiki/index.php";
var wgVariantArticlePath = false;
var wgActionPaths = [];
var wgServer = "http://localhost/";
var wgCanonicalNamespace = "";
var wgCanonicalSpecialPageName = false;
var wgNamespaceNumber = 0;
var wgPageName = "Data_Conversion_and_Preparation";
var wgTitle = "Data Conversion and Preparation";
var wgAction = "view";
var wgArticleId = "15";
var wgIsArticle = true;
var wgUserName = null;
var wgUserGroups = null;
var wgUserLanguage = "en";
var wgContentLanguage = "en";
var wgBreakFrames = false;
var wgCurRevisionId = "664";
var wgVersion = "1.13.5";
var wgEnableAPI = true;
var wgEnableWriteAPI = false;
var wgRestrictionEdit = [];
var wgRestrictionMove = [];
/*]]>*/</script>
                
		<script type="text/javascript" src="../mediawiki/skins/common/wikibitsfa7c.js?164"><!-- wikibits js --></script>
		<!-- Head Scripts -->
		<script type="text/javascript" src="../mediawiki/skins/common/ajaxfa7c.js?164"></script>
		<script type="text/javascript" src="http://localhost/mediawiki/index.php?title=-&amp;action=raw&amp;gen=js&amp;useskin=monobook"><!-- site js --></script>
		<style type="text/css">/*<![CDATA[*/
@import "http://localhost/mediawiki/index.php?title=MediaWiki:Common.css&amp;usemsgcache=yes&amp;action=raw&amp;ctype=text/css&amp;smaxage=18000";
@import "http://localhost/mediawiki/index.php?title=MediaWiki:Monobook.css&amp;usemsgcache=yes&amp;action=raw&amp;ctype=text/css&amp;smaxage=18000";
@import "http://localhost/mediawiki/index.php?title=-&amp;action=raw&amp;gen=css&amp;maxage=18000&amp;useskin=monobook";
/*]]>*/</style>
	</head>
<body class="mediawiki ns-0 ltr page-Data_Conversion_and_Preparation">
	<div id="globalWrapper">
		<div id="column-content">
	<div id="content">
		<a name="top" id="top"></a>
				<h1 class="firstHeading">Data Conversion and Preparation</h1>
		<div id="bodyContent">
			<h3 id="siteSub">From DLXS Documentation</h3>
			<div id="contentSub"></div>
									<div id="jump-to-nav">Jump to: <a href="#column-one">navigation</a>, <a href="#searchInput">search</a></div>			<!-- start content -->
			<p><a href="DLXS_Wiki.html" title="DLXS Wiki">Main Page</a> &gt; Data Conversion and Preparation
</p>
<hr />
<p>Other relevant data preparation topics include the following:
</p>
<ul><li> <a href="Working_with_Unicode.html" title="Working with Unicode">Working with Unicode</a>
</li><li> <a href="Working_with_XML_%26_XSLT.html" title="Working with XML &amp; XSLT">Working with XML &amp; XSLT</a>
</li></ul>
<table id="toc" class="toc" summary="Contents"><tr><td><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1"><a href="#Converting_and_Preparing_Your_Data"><span class="tocnumber">1</span> <span class="toctext">Converting and Preparing Your Data</span></a></li>
<li class="toclevel-1"><a href="#Unicode.2C_XML.2C_and_Normalization"><span class="tocnumber">2</span> <span class="toctext">Unicode, XML, and Normalization</span></a>
<ul>
<li class="toclevel-2"><a href="#Determining_the_Character_Encodings_Present_in_Your_Data"><span class="tocnumber">2.1</span> <span class="toctext">Determining the Character Encodings Present in Your Data</span></a></li>
<li class="toclevel-2"><a href="#Converting_Those_Character_Encodings_to_UTF-8"><span class="tocnumber">2.2</span> <span class="toctext">Converting Those Character Encodings to UTF-8</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Workshop_Materials:_Test_Driving_the_Tools"><span class="tocnumber">3</span> <span class="toctext">Workshop Materials: Test Driving the Tools</span></a>
<ul>
<li class="toclevel-2"><a href="#More_Documentation"><span class="tocnumber">3.1</span> <span class="toctext">More Documentation</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Normalization_and_Converting_SGML_to_XML"><span class="tocnumber">4</span> <span class="toctext">Normalization and Converting SGML to XML</span></a>
<ul>
<li class="toclevel-2"><a href="#More_Documentation_2"><span class="tocnumber">4.1</span> <span class="toctext">More Documentation</span></a></li>
</ul>
</li>
<li class="toclevel-1"><a href="#Using_Unconverted_Collections_with_Current_Middleware"><span class="tocnumber">5</span> <span class="toctext">Using Unconverted Collections with Current Middleware</span></a></li>
</ul>
</td></tr></table><script type="text/javascript"> if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } </script>
<a name="Converting_and_Preparing_Your_Data"></a><h2><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=1" title="Edit section: Converting and Preparing Your Data">edit</a>]</span> <span class="mw-headline">Converting and Preparing Your Data</span></h2>
<p>For many collections, converting and preparing data is the most time-consuming and difficult part of mounting the collection online. Because each conversion project is specific to your material and cannot be easily generalized, DLXS does not formally support mechanisms for converting data to various formats. Nevertheless, we do provide some documentation on strategies, tools, and methods that we have found helpful for data conversion. Some of this documentation is class-specific, and some deals with more general Unicode and XML issues. 
</p><p>For Image Class and Bib Class, the class-specific links below provide fairly straightforward strategies and some tools for converting the fielded data typical in those classes.
</p><p>In some cases, however, you must further modify your data to work effectively with DLXS. For example, converting to the Text Class DTD and inserting “nodes” in Text Class documents. (link) For Text Class, DLPS does not have any preferred methods or quick and easy tools for the conversion process. Only you, after looking at your texts and your encoding practices, can do the intellectual work required to convert the texts to support the necessary Text Class data structure (link?) You should do this with the tools you are most comfortable using, whether they are macros in your favorite editor, perl scripts in you have strong programming skills, OmniMark, or XSLT if your source files are currently or can be converted to XML. The Text Class documentation linked below describes a fairly detailed XSLT strategy, using freely-available or ubiquitous tools. 
</p><p><br />
For non-unicode specific information on data preparation for individual classes, see the following: [also edit in Working with Unicode]
</p>
<ul><li> Preparing Text Class Data for Index Building / Converting Collections to Text Class
</li><li> Image Class—where is this section? / Image Class Data Loading: My SQL
</li><li> Transforming Bibliographic Class Files
</li><li> <a href="Mounting_a_Finding_Aids_Collection.html#:Preparing_Data_and_Directories" title="Mounting a Finding Aids Collection">Mounting a Finding Aids Collection: Preparing Data and Directories</a>
</li></ul>
<p>For other encoding information, see the following:
</p>
<ul><li> <a href="Working_with_Unicode.html" title="Working with Unicode">Working with Unicode</a> 
</li><li> <a href="Working_with_XML_%26_XSLT.html" title="Working with XML &amp; XSLT">Working with XML &amp; XSLT</a>
</li></ul>
<a name="Unicode.2C_XML.2C_and_Normalization"></a><h2><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=2" title="Edit section: Unicode, XML, and Normalization">edit</a>]</span> <span class="mw-headline">Unicode, XML, and Normalization</span></h2>
<p>To make the most of Text Class and Finding Aids Class in DLXS Release 12 and up, you will want to convert or otherwise handle the character entities, numeric entities, or Latin 18-bit characters that have been the staples of SGML (and XML, despite the default encoding of UTF-8) for so long. This is separate from the conversion of Text Class materials to the Text Class DTD; even with finding aids that are already in XML, you will probably need to do some testing of character encodings, conversion of these encodings to UTF-8, normalization, and conversion of SGML to XML (strange but true).
</p><p>For more Unicode-specific information, see <a href="Working_with_Unicode.html" title="Working with Unicode">Working with Unicode</a>.
</p><p><br />
</p>
<a name="Determining_the_Character_Encodings_Present_in_Your_Data"></a><h3><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=3" title="Edit section: Determining the Character Encodings Present in Your Data">edit</a>]</span> <span class="mw-headline">Determining the Character Encodings Present in Your Data</span></h3>
<p>There are a number of possibilities you may encounter:
</p>
<pre>  1. Plain ASCII (aka the Basic Latin block)
  2. Character entity references (ISO and otherwise)
  3. Numeric character references (decimal and/or hexadecimal)
  4. Latin 1 characters
  5. UTF-8 characters
</pre>
<p>You may very well find a mixture of 1, 2, 3, and 4 or even 2, 3, and 5 in the wild, simply because many encoders are not clear on what they should be doing with special characters. One hopes you will not encounter a document with a mixture of Latin 1 and UTF-8 characters, although it's possible that misidentified files could end up concatenated together and create such a mess.
There are a number of tools you can use to identify what you have before you:
</p><p><b>findentities.pl</b>
A perl script written by Phil that is part of the DLXS package, it prints the names and frequencies of the entities (CERs and NCRs) it encounters. Fairly quick, regardless of the size of the file. Can be run on more than one file at once, which is handy if you have a batch of texts.
</p><p><b>xpatutf8check</b>
Another perl script written by Phil, it exists to answer the question, "Will xpatu index this?" It will report the line number of the first non-UTF character it encounters when it has failed and it runs very quickly, so it's great as a first step in checking your material, but it is not authoritative enough to identify all of the problems you may have.
</p><p><b>jhove</b>
The JSTOR/Harvard Object Validation Environment has a UTF-8 module that reports whether your document is or is not valid UTF-8, and which Unicode blocks are contained in the document. Can be slow checking large documents, but very informative. Available at <a href="http://hul.harvard.edu/jhove/" class="external free" title="http://hul.harvard.edu/jhove/" rel="nofollow">http://hul.harvard.edu/jhove/</a> and invoked with
jhove -c /l/local/jhove/conf/jhove.conf -m utf8-hul file.xml
</p><p><b>utf8chars</b>
Yet another perl script written by Phil, it identifies the characters used in a document and the Unicode blocks to which they belong. It assumes your document is UTF-8 and will report each instance (by line number) where a non-UTF character is encountered. Because it is identifying and counting each character in a document, it is rather slow, but very very useful. Runs on one file at a time and prints to standard out, but can be invoked through a foreach to check many files in one command.
</p>
<a name="Converting_Those_Character_Encodings_to_UTF-8"></a><h3><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=4" title="Edit section: Converting Those Character Encodings to UTF-8">edit</a>]</span> <span class="mw-headline">Converting Those Character Encodings to UTF-8</span></h3>
<p>If you have a mixed bag of encodings and entities in your documents, there's a definite order in which you want to approach the conversion task, to avoid having a mixture of Latin 1 and UTF-8 in one document at any point in the transformation.
</p>
<ol><li> First, if you have Latin 1 characters like â, run iconv, part of the Gnu C library, to convert files from one encoding to another.<br /><br /><code>iconv -f iso88591 -t utf8 oldfile &gt; newfile</code><br /><br />
</li><li> Next, convert character entity references like &amp;acirc; using isocer2utf8, a perl script written by Phil to convert character entity references to UTF-8 characters. Although it references ISO in the name, it's been expanded to handle all the CERs we've encountered, including TEI Greek and the Chadwyck-Healey custom entities.<br /><br /><code>/l1/bin/t/text/isocer2utf8 oldfile &gt; newfile</code><br /><br />
</li><li> Finally, if you have numeric character references like &#226; or &amp;amp#xE2;, run ncr2utf8, also written by Phil, to convert decimal and hexadecimal entities to UTF-8 characters.<br /><br /><code>/l1/bin/t/text/ncr2utf8 oldfile &gt; newfile</code>&lt;/p&gt;
</li></ol>
<p>This would be a good point to run findentities.pl again to see what (if anything) you have left, and to re-validate using jhove or utf8chars to ensure that you have done no harm.
</p>
<a name="Workshop_Materials:_Test_Driving_the_Tools"></a><h2><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=5" title="Edit section: Workshop Materials: Test Driving the Tools">edit</a>]</span> <span class="mw-headline">Workshop Materials: Test Driving the Tools</span></h2>
<p>In the directory <code>/l1/workshop-samples/sooty</code>, you will find four sample files that we'll examine for character encoding and then convert to UTF-8. Copy these to your own directory -- they are completely expendable and won't serve a purpose in tomorrow's Text Class implementation. They are merely illustrative of all the possibilities you might encounter and how you may want to handle them.
First, we'll look at which character or numeric entities, if any, are used in these documents.
</p><p><code>
</p>
<pre>   foreach file (findaid*)
   echo $file 
   $DLXSROOT/bin/t/text/findEntities.pl $file 
   end
</pre>
<p></code>
</p><p><code>
</p>
<pre>   foreach file (text*)
   echo $file 
   $DLXSROOT/bin/t/text/findEntities.pl $file 
   end
</pre>
<p></code>
</p><p>Since most of you are set up for bash, here are the same commands in that shell:
</p><p><code>
</p>
<pre>   for file in findaid*
   do
   echo $file 
   $DLXSROOT/bin/t/text/findEntities.pl $file
   done
</pre>
<p></code>
</p><p><code>
</p>
<pre>   for file in text*
   do
   echo $file 
   $DLXSROOT/bin/t/text/findEntities.pl $file
   done
</pre>
<p></code>
</p><p>We have some CERs and NCRs to deal with, aside from the five XML-approved ent&lt;/code&gt; and <code>ncr2utf</code>. Next, we'll see what characters we have (Latin 1? UTF-8? something else?). We'll run through all three tools, just for the sake of completeness, in the order of speediness and terseness.
</p><p><code>
</p>
<pre>   foreach file (findaid*)
   echo $file 
   xpatutf8check $file 
   end
</pre>
<pre>   foreach file (text*)
   echo $file 
   xpatutf8check  $file 
   end
</pre>
<p></code>
</p><p>Since most of you are set up for bash, here are the same commands in that shell:
</p><p><code>
</p>
<pre>   for file in findaid*
   do
   echo $file 
   xpatutf8check  $file
   done
</pre>
<pre>   for file in text*
   do
   echo $file 
   xpatutf8check  $file
   done
</pre>
<p></code>
</p><p>We now know that both the text files are either UTF-8 or plain ASCII (because of the output of these two tests), but there's a problem with one of the finding aids. jhove will tell us a bit more about our materials. You'll note we don't need to echo the filename as that's part of the jhove report. You'll also notice jhove is not so fast.
</p><p><code>
</p>
<pre>   foreach file (findaid*)
   jhove -c /l/local/jhove/conf/jhove.conf -m utf8-hul  $file 
   end
</pre>
<pre>   foreach file (text*)
   jhove -c /l/local/jhove/conf/jhove.conf -m utf8-hul  $file 
   end
</pre>
<p></code>
</p><p>If you are a bash aficionado, here are the same commands in that shell:
</p><p><code>
</p>
<pre>   for file in findaid*
   do
   jhove -c /l/local/jhove/conf/jhove.conf -m utf8-hul  $file
   done
</pre>
<pre>   for file in text*
   do
   jhove -c /l/local/jhove/conf/jhove.conf -m utf8-hul  $file
   done
</pre>
<p></code>
</p><p>So, the second file in each set is plain ASCII (the Basic Latin block) with entities, the first finding aid is not UTF-8, and the first text file is. Let's look a bit more at the two non-ASCII files with the slowest and most verbose tool of them all. We're not doing a foreach this time, but we wouldn't need to echo the filename either, as it is again part of what the tool reports.
</p><p><code>
</p>
<pre>   utf8chars findaid1.xml
   utf8chars text1.xml
</pre>
<p></code>
</p><p>We can see the exact problem with <code>findaid1.xml</code> -- there's an 8-bit Latin 1 e acute before Boekeloo on line 37. We also can see all the UTF-8 characters in <code>text1.xml</code> -- this is the sort of information that is useful when time comes to map characters and encodings in the xpatu data dictionary.
</p><p>Now that we know which items need what character treatments, we'll convert them. text1.xml is completely fine, so we'll leave it as is. <code>findaid1.xml</code> has the one Latin 1 character, so we'll use iconv to convert it to UTF-8. It had no entities of any kind, so we'll be done with it after this step.
</p><p><code>
</p>
<pre>   iconv -f iso88591 -t utf8 findaid1.xml &gt; findaid1.xml.utf
</pre>
<p></code>
</p><p>Next, <code>findaid2.xml</code> had numeric character references. It is fine and can be indexed as-is, but users would need to search for the hexadecimal string in the midst of words ( &#xe9; for é, for example). So, we'll use ncr2utf to convert the entities into the characters. WARNING! &#x26; is the ampersand (as is &#38;) -- if you convert these to the character, you will run into validation problems down the road, as bare ampersands are not permitted in XML. Don't get carried away!
</p><p><code>
</p>
<pre>   ncr2utf8 findaid2.xml &gt; findaid2.xml.utf
</pre>
<p></code>
</p><p>Finally, <code>text2.sgm</code> has ISO character entity references (from Latin 1, Greek, and Publishing) that need to be converted to UTF-8 with <code>isocer2utf</code>.
</p><p><code>
</p>
<pre>   isocer2utf8 text2.sgm &gt; text2.sgm.utf
</pre>
<p></code>
</p><p>Note that the ampersand CER was not processed. This is perfectly correct.
</p>
<a name="More_Documentation"></a><h3><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=6" title="Edit section: More Documentation">edit</a>]</span> <span class="mw-headline">More Documentation</span></h3>
<ul><li> <a href="Working_with_Unicode.html" title="Working with Unicode">Working with Unicode</a>
</li><li> <a href="http://www.zvon.org/other/charSearch/PHP/search.php" class="external text" title="http://www.zvon.org/other/charSearch/PHP/search.php" rel="nofollow">Zvon Character Search</a>
</li><li> <a href="http://www.alanwood.net/unicode/" class="external text" title="http://www.alanwood.net/unicode/" rel="nofollow">Unicode and Multilingual Support in HTML, Fonts, Web Browsers and Other Applications</a>
</li></ul>
<a name="Normalization_and_Converting_SGML_to_XML"></a><h2><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=7" title="Edit section: Normalization and Converting SGML to XML">edit</a>]</span> <span class="mw-headline">Normalization and Converting SGML to XML</span></h2>
<p>Many of you may be in a position where you'll want to be converting your SGML files to XML. Many of you will be fortunate enough to have files already in XML -- say, finding aids in EAD 2002. However, these will have to be normalized, too, to avoid problems with xpatu and xmlrgn down the road by ensuring that all the attributes are in the same order as specified in the DTD. Because of known but uncorrected problems in the normalization tools, you will end up with SGML and will need to convert that to XML.
</p><p>Because the file we want to work with is now UTF-8, we need to set some environment variables for the tools from the sp package to let them know this is UTF-8. It doesn't matter that you've set your puTTy window to UTF-8, if you are using osx, osgmlnorm, or onsgmls, you must set your environment properly. For example:
</p><p><code>
</p>
<pre>   setenv SP_CHARSET_FIXED YES
   setenv SP_ENCODING utf-8
</pre>
<p></code>
</p><p>For those of you in bash, it's
<code>
</p>
<pre>   export SP_CHARSET_FIXED=YES
   export SP_ENCODING=utf-8
</pre>
<p></code>
</p><p>First we normalize, invoking a declaration to handle the non-SGML UTF-8 characters without claiming that the material itself is XML.
</p><p><code>
</p>
<pre>  osgmlnorm $DLXSROOT/misc/sgml/xmlentities.dcl sample.inp text2.sgm.utf &gt; text2.sgm.norm
</pre>
<p></code>
</p><p>Now I'll test the output with one of the UTF-8 tools to make sure that it's come through unscathed, and with findEntities.pl to see what has happened with the remaining XML-friendly entities, and it's fine. Now to convert our SGML to XML using osx.
</p><p><code>
</p>
<pre>   osx -x no-nl-in-tag -x empty -E 500 -f errors $DLXSROOT/misc/sgml/xmlentities.dcl sample.inp          text2.sgm.norm &gt; text2.xml
</pre>
<p></code>
</p><p>Again I'll test the output with one of the UTF-8 tools to make sure that it's come through unscathed, and with findEntities.pl to see what has happened with the remaining XML-friendly entities, and again it's fine.
</p><p>Just for fun, we'll normalize the files already in XML, just to show that things get changes from XML to SGML against their will.
</p><p><code>
</p>
<pre>   osgmlnorm $DLXSROOT/misc/sgml/xml.dcl $DLXSROOT/prep/s/sampletc_utf8/sampletc_utf8.text.inp   text1.xml &gt; text1.xml.norm
   osx -x no-nl-in-tag -x empty -E 5000 -f error $DLXSROOT/misc/sgml/xml.dcl     $DLXSROOT/prep/s/sampletc_utf8/sampletc_utf8.text.inp  text1.xml.norm &gt; text1.xml.norm.xml
</pre>
<p></code>
</p>
<a name="More_Documentation_2"></a><h3><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=8" title="Edit section: More Documentation">edit</a>]</span> <span class="mw-headline">More Documentation</span></h3>
<ul><li> DLXS Unicode Data Preparation and Online Presentation Issues
</li><li> <a href="http://linuxcommand.org/man_pages/osgmlnorm1.html" class="external text" title="http://linuxcommand.org/man_pages/osgmlnorm1.html" rel="nofollow">osgmlnorm Documentation</a>
</li><li> <a href="http://linuxcommand.org/man_pages/osx1.html" class="external text" title="http://linuxcommand.org/man_pages/osx1.html" rel="nofollow">osx Documentation</a>
</li><li> <a href="http://www.jclark.com/sp/" class="external text" title="http://www.jclark.com/sp/" rel="nofollow">James Clark's Original sp Documentation</a>
</li></ul>
<a name="Using_Unconverted_Collections_with_Current_Middleware"></a><h2><span class="editsection">[<a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit&amp;section=9" title="Edit section: Using Unconverted Collections with Current Middleware">edit</a>]</span> <span class="mw-headline">Using Unconverted Collections with Current Middleware</span></h2>
<p>Recognizing that there may be situations when you do not wish to migrate a collection to XML and UTF-8 immediately, there are mechanisms built into Release 13 to allow the middleware to handle SGML-style empty elements (aka singletons), Latin 1 characters, and character entity references. How do you make this happen? 
</p>
<ul><li> In DlpsUtils.pm, there is a subroutine called Sgml2XmlFilter that has a hard-coded list of empty elements (<code>&lt;PB&gt;, &lt;LB&gt;, &lt;CAESURA&gt;</code>, etc.) that are converted upon discovery to XML-style (<code>&lt;PB/&gt;, &lt;LB/&gt;, &lt;CAESURA/&gt;</code>, etc.). 
</li><li> There is also a feature that converts Latin 1 (ISO-8859-1) characters to UTF-8. This subroutine comes into play if the locale field in collmgr is not set to en_US.UTF-8 (locale used to be optional but is now required if you are using UTF-8 and xpatu). 
</li></ul>
<p>In order to declare your entities, you need to put a file called <code>entitiesdoctype.chnk</code> in the web directory for your collection, declaring the entities like so:
</p><p><code>
</p>
<pre>   &lt;!DOCTYPE Top [
   &lt;!ENTITY Dstrok   "&#x110;"&gt;
   &lt;!ENTITY Sacute   "&#x15a;"&gt;
   &lt;!ENTITY Scaron   "&#352;"&gt;
   &lt;!ENTITY Ubreve   "&#x16c;"&gt;
   &lt;!ENTITY Zdot     "&#x17b;"&gt;
   ]&gt;
</pre>
<p></code>
</p><p>That being the case, why would anyone ever bother to go through the trouble of converting their material? First, the value of having UTF-8 is apparent if you have material that used more than one entity set (and even the lowliest collections have both an e acute and an em-dash in them somewhere). Now that &mdash; is one character that can be mapped to a space in the data dictionary like other punctuation, phrases that were obscured in searches now turn up, and characters that we used to flatten (transforming ā to a, for example) can be displayed. Second, this facility comes at a cost. All of the material returned needs to be run through this filter, which will take some time. In a results list, the lag is negligable, but in larger sections of text, it could be noticeable. Finally, some confusion might arise when a user cuts and pastes material he received as a result and cannot retrieve it again, because the results and input are UTF-8 (which is the encoding of the search form) but the material being searched is not.
</p><p><br />
<a href="#top" title="">Top</a>
</p>
<!-- 
NewPP limit report
Preprocessor node count: 53/1000000
Post-expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Expensive parser function count: 0/100
-->

<!-- Saved in parser cache with key mediawiki_dlxsdocs_15:pcache:idhash:15-0!1!0!!en!2 and timestamp 20250202211311 -->
<div class="printfooter">
Retrieved from "<a href="Data_Conversion_and_Preparation.html">http://localhost/wiki/Data_Conversion_and_Preparation</a>"</div>
						<!-- end content -->
			<div class="visualClear"></div>
		</div>
	</div>
		</div>
		<div id="column-one">
	<div id="p-cactions" class="portlet">
		<h5>Views</h5>
		<div class="pBody">
			<ul>
	
				 <li id="ca-nstab-main" class="selected"><a href="Data_Conversion_and_Preparation.html" title="View the content page [c]" accesskey="c">Page</a></li>
				 <li id="ca-talk" class="new"><a href="http://localhost/mediawiki/index.php?title=Talk:Data_Conversion_and_Preparation&amp;action=edit" title="Discussion about the content page [t]" accesskey="t">Discussion</a></li>
				 <li id="ca-edit"><a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=edit" title="You can edit this page.&#10;Please use the preview button before saving. [e]" accesskey="e">Edit</a></li>
				 <li id="ca-history"><a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;action=history" title="Past versions of this page. [h]" accesskey="h">History</a></li>			</ul>
		</div>
	</div>
	<div class="portlet" id="p-personal">
		<h5>Personal tools</h5>
		<div class="pBody">
			<ul>
				<li id="pt-login"><a href="http://localhost/mediawiki/index.php?title=Special:UserLogin&amp;returnto=Data_Conversion_and_Preparation" title="You are encouraged to log in, it is not mandatory however. [o]" accesskey="o">Log in / create account</a></li>
			</ul>
		</div>
	</div>
	<div class="portlet" id="p-logo">
		<a style="background-image: url(../dlxs15/uploads/DLXS_logo2.gif);" href="Main_Page.html" title="Visit the Main Page [z]" accesskey="z"></a>
	</div>
	<script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script>
	<div class='generated-sidebar portlet' id='p-navigation'>
		<h5>Navigation</h5>
		<div class='pBody'>
			<ul>
				<li id="n-mainpage-description"><a href="Main_Page.html">Main Page</a></li>
				<li id="n-portal"><a href="DLXS_Documentation_Community_Portal.html" title="About the project, what you can do, where to find things">Community portal</a></li>
				<li id="n-currentevents"><a href="DLXS_Documentation_Current_events.html" title="Find background information on current events">Current events</a></li>
				<li id="n-recentchanges"><a href="Special_RecentChanges.html" title="The list of recent changes in the wiki. [r]" accesskey="r">Recent changes</a></li>
				<li id="n-randompage"><a href="Mounting_a_Bib_Class_Collection.html" title="Load a random page [x]" accesskey="x">Random page</a></li>
				<li id="n-help"><a href="Help_Contents.html" title="The place to find out.">Help</a></li>
			</ul>
		</div>
	</div>
	<div id="p-search" class="portlet">
		<h5><label for="searchInput">Search</label></h5>
		<div id="searchBody" class="pBody">
			<form action="http://localhost/wiki/Special:Search" id="searchform"><div>
				<input id="searchInput" name="search" type="text" title="Search DLXS Documentation [f]" accesskey="f" value="" />
				<input type='submit' name="go" class="searchButton" id="searchGoButton"	value="Go" title="Go to a page with this exact name if exists" />&nbsp;
				<input type='submit' name="fulltext" class="searchButton" id="mw-searchButton" value="Search" title="Search the pages for this text" />
			</div></form>
		</div>
	</div>
	<div class="portlet" id="p-tb">
		<h5>Toolbox</h5>
		<div class="pBody">
			<ul>
				<li id="t-whatlinkshere"><a href="Special_WhatLinksHere/Data_Conversion_and_Preparation.html" title="List of all wiki pages that link here [j]" accesskey="j">What links here</a></li>
				<li id="t-recentchangeslinked"><a href="Special_RecentChangesLinked/Data_Conversion_and_Preparation.html" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li>
<li id="t-specialpages"><a href="Special_SpecialPages.html" title="List of all special pages [q]" accesskey="q">Special pages</a></li>
				<li id="t-print"><a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				<li id="t-permalink"><a href="http://localhost/mediawiki/index.php?title=Data_Conversion_and_Preparation&amp;oldid=664" title="Permanent link to this version of the page">Permanent link</a></li>			</ul>
		</div>
	</div>
		</div><!-- end of the left (by default at least) column -->
			<div class="visualClear"></div>
			<div id="footer">
				<div id="f-poweredbyico"><a href="http://www.mediawiki.org/"><img src="../mediawiki/skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" /></a></div>
			<ul id="f-list">
				<li id="lastmod"> This page was last modified on 13 August 2007, at 22:15.</li>
				<li id="viewcount">This page has been accessed 6,704 times.</li>
				<li id="privacy"><a href="DLXS_Documentation_Privacy_policy.html" title="DLXS Documentation:Privacy policy">Privacy policy</a></li>
				<li id="about"><a href="DLXS_Documentation_About.html" title="DLXS Documentation:About">About DLXS Documentation</a></li>
				<li id="disclaimer"><a href="DLXS_Documentation_General_disclaimer.html" title="DLXS Documentation:General disclaimer">Disclaimers</a></li>
			</ul>
		</div>
</div>

		<script type="text/javascript">if (window.runOnloadHook) runOnloadHook();</script>
<!-- Served in 0.117 secs. --></body>
<!-- Mirrored from localhost/wiki/Data_Conversion_and_Preparation by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 03 Feb 2025 00:25:50 GMT -->
</html>
